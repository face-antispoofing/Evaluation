{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import ViTHybridImageProcessor, ViTHybridForImageClassification\n",
    "from transformers import AutoImageProcessor, SwinForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Visual transformer model from https://huggingface.co/microsoft/swin-tiny-patch4-window7-224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define data transforms (you can modify these based on your needs)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a consistent size\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Define paths to your data folders\n",
    "train_data_dir = \"/data/Desktop/trainset/\"\n",
    "val_data_dir = \"/data/Desktop/validation/\"\n",
    "test_data_dir = \"/data/Desktop/testset/\"\n",
    "\n",
    "# Load your training, validation, and test datasets using ImageFolder\n",
    "train_dataset = ImageFolder(root=train_data_dir, transform=transform)\n",
    "val_dataset = ImageFolder(root=val_data_dir, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_data_dir, transform=transform)\n",
    "\n",
    "# If you want to access the labels for train, validation, and test datasets:\n",
    "train_labels = train_dataset.targets\n",
    "val_labels = val_dataset.targets\n",
    "test_labels = test_dataset.targets\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8  # Adjust the batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a binary classification head\n",
    "class BinaryClassificationHead(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BinaryClassificationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Modify the Swin Transformer model for binary classification\n",
    "classifier_head = BinaryClassificationHead(768, 32)  # Adjust input size as needed\n",
    "model.classifier = classifier_head\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming model, train_loader, val_loader, criterion, and optimizer are defined\n",
    "\n",
    "num_epochs = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "patience = 5\n",
    "verbose = True\n",
    "delta = 0.001  # Set an appropriate delta value\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data).logits\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_data, val_labels in val_loader:\n",
    "            val_data, val_labels = val_data.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_data).logits\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1).float()).item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss - delta:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Save the model if validation loss is improved\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    # Append losses for plotting\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "# Plotting the training and validation curves\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Validation set for EER calculation\n",
    "model.eval()\n",
    "val_labels, val_scores = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in val_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        outputs = model(data).logits\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "        val_scores.extend(torch.sigmoid(outputs).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Equal Error Rate (EER) on the validation set\n",
    "fpr, tpr, thresholds = roc_curve(val_labels, val_scores, pos_label=1)\n",
    "eer_threshold = thresholds[np.argmin(np.abs(tpr - (1 - fpr)))]\n",
    "eer = 1 - tpr[np.argmin(np.abs(tpr - (1 - fpr)))]\n",
    "print(f'Validation EER: {eer*100:.4f}, EER Threshold: {eer_threshold:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store test scores and labels\n",
    "test_labels, test_scores = [], []\n",
    "\n",
    "# Make predictions on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        outputs = model(data).logits\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_scores.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "# Calculate the HTER on the testing set using the EER threshold\n",
    "threshold = eer_threshold\n",
    "predicted_labels = [1 if score > threshold else 0 for score in test_scores]\n",
    "\n",
    "false_acceptance = sum(1 for i in range(len(predicted_labels)) if predicted_labels[i] == 1 and test_labels[i] == 0)\n",
    "false_rejection = sum(1 for i in range(len(predicted_labels)) if predicted_labels[i] == 0 and test_labels[i] == 1)\n",
    "\n",
    "total_samples = len(test_labels)\n",
    "hter = ((false_acceptance + false_rejection) / (2 * total_samples)) * 100\n",
    "print(f\"HTER: {hter:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AUC on the testing set\n",
    "auc_score = roc_auc_score(test_labels, test_scores)\n",
    "print(f\"AUC on the testing set: {auc_score*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

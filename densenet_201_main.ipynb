{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82283e30-6d80-4737-a06b-be2393954828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np \n",
    "import timm\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c33e1-a75d-40c1-99e1-20ff70b835ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for Python random module\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# Define paths to your data folders\n",
    "train_data_dir = '/u/45/muhammu2/data/Desktop/spoofing/train/'\n",
    "val_data_dir = '/u/45/muhammu2/data/Desktop/spoofing/validation/'\n",
    "test_data_dir = '/u/45/muhammu2/data/Desktop/spoofing/test/'\n",
    "\n",
    "# Example transformations (customize based on your requirements)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load your training, validation, and test datasets using ImageFolder\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=transform)\n",
    "\n",
    "# Print the number of images in each dataset\n",
    "print(f\"Number of images in the training set: {len(train_dataset)}\")\n",
    "print(f\"Number of images in the validation set: {len(val_dataset)}\")\n",
    "print(f\"Number of images in the test set: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoader for training, validation, and test sets\n",
    "batch_size = 4  # Adjust according to your needs\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12cf0d-32ca-4641-9a27-a0d9bbcdc95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SimpleDenseNet model (as defined previously)\n",
    "class SimpleDenseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleDenseNet, self).__init__()\n",
    "        self.model = timm.create_model('densenet201', pretrained=True)\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the custom DenseNet model\n",
    "num_classes = 2  # Binary classification\n",
    "model = SimpleDenseNet(num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc5fdd-268d-4bd1-9633-9526eb658339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs =  5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize EER and threshold variables\n",
    "eer = None\n",
    "threshold = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            scores = torch.nn.functional.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "    # Calculate EER once after training on the validation set\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_scores, pos_label=1)\n",
    "\n",
    "    # Check for NaN values in the arrays\n",
    "    if any(np.isnan(fpr)) or any(np.isnan(tpr)) or any(np.isnan(thresholds)):\n",
    "        print(\"Error: NaN values encountered in fpr, tpr, or thresholds during EER calculation.\")\n",
    "        # Handle this case accordingly\n",
    "    else:\n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        threshold = thresholds[np.nanargmin(np.abs(fpr - eer))]\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation EER: {eer * 100:.2f}%\")\n",
    "        print(f\"Validation EER Threshold: {threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afbeed4-3810-492a-93cc-14453b2d1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if eer is not None and threshold is not None:\n",
    "    model.eval()\n",
    "\n",
    "    genuine_scores = []\n",
    "    impostor_scores = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            scores_test = torch.nn.functional.softmax(outputs, dim=1)[:, 1].cpu().detach().numpy()\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            impostor_scores.extend(scores_test[labels.cpu() == 0])  # Impostor scores\n",
    "            genuine_scores.extend(scores_test[labels.cpu() == 1])   # Genuine scores\n",
    "\n",
    "    # Apply threshold to impostor scores to get binary predictions\n",
    "    binary_predictions = (np.array(impostor_scores) > threshold).astype(int)\n",
    "\n",
    "    # Calculate FAR and FRR\n",
    "    false_acceptance = sum(binary_predictions) / len(binary_predictions)\n",
    "    false_rejection = sum(genuine_score <= threshold for genuine_score in genuine_scores) / len(genuine_scores)\n",
    "\n",
    "    # Calculate HTER\n",
    "    hter = 0.5 * (false_acceptance + false_rejection)\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(np.array(true_labels), np.concatenate([impostor_scores, genuine_scores]))\n",
    "\n",
    "    print(f\"Test HTER: {hter * 100:.2f}%\")\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "else:\n",
    "    print(\"Error: EER and threshold not calculated during validation.\")\n",
    "    # Calculate the AUC on the testing set\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc_score = roc_auc_score(labels, scores_test.cpu())\n",
    "    print(f\"AUC on the testing set: {auc_score*100:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
